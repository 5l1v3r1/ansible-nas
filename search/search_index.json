{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"After getting burned by broken FreeNAS updates one too many times, I figured I could do a much better job myself using just a stock Ubuntu install, some clever Ansible config and a bunch of Docker containers. Ansible-NAS was born! Features An awesome dashboard to your home server (Heimdall) Any number of Samba shares for you to store your stuff A BitTorrent client Various media management tools - Sonarr, Sickchill, CouchPotato, Radarr Media streaming via Plex, Emby or MiniDLNA Music streaming with Airsonic A Dropbox replacement via Nextcloud Various ways to see stats about your NAS - Glances, dashboards in Grafana A backup tool - allows scheduled backups to Amazon S3, OneDrive, Dropbox etc An IRC bouncer Source control with Gitea SSL secured external access to some applications via Traefik A Docker host with Portainer management - run anything that's shipped as a Docker container Getting Started Head to installation if you're ready to roll, or to testing if you want to spin up a test Virtual Machine first. Once you're done, check out the post-installation steps. If this is all very confusing, there is also an overview of the project and what is required for complete beginners. If you're only confused abot ZFS, we'll help you get started as well.","title":"Home"},{"location":"#features","text":"An awesome dashboard to your home server (Heimdall) Any number of Samba shares for you to store your stuff A BitTorrent client Various media management tools - Sonarr, Sickchill, CouchPotato, Radarr Media streaming via Plex, Emby or MiniDLNA Music streaming with Airsonic A Dropbox replacement via Nextcloud Various ways to see stats about your NAS - Glances, dashboards in Grafana A backup tool - allows scheduled backups to Amazon S3, OneDrive, Dropbox etc An IRC bouncer Source control with Gitea SSL secured external access to some applications via Traefik A Docker host with Portainer management - run anything that's shipped as a Docker container","title":"Features"},{"location":"#getting-started","text":"Head to installation if you're ready to roll, or to testing if you want to spin up a test Virtual Machine first. Once you're done, check out the post-installation steps. If this is all very confusing, there is also an overview of the project and what is required for complete beginners. If you're only confused abot ZFS, we'll help you get started as well.","title":"Getting Started"},{"location":"contributing/","text":"Contributing Contributing to Ansible-NAS is easy! Add your functionality, then raise a pull request on GitHub. A few things to bear in mind: Restrict pull requests to one piece of functionality or bugfix at a time. Test your new functionality or bugfix using the included tests/test-vagrant.sh script to spin up a test VM. Run ansible-lint against the playbook before committing. (There is a VSCode task set up to run the right command for you) Ensure that your PR only changes files required for your functionality or bugfix. If you're adding a new application: Ensure that the new application is disabled by default. Add a documentation page to docs/applications/ - use an existing application as an example. Add the frontend port to docs/configuration/application_ports.md , ensuring you've not clashed with an existing application.","title":"Contributing"},{"location":"contributing/#contributing","text":"Contributing to Ansible-NAS is easy! Add your functionality, then raise a pull request on GitHub. A few things to bear in mind: Restrict pull requests to one piece of functionality or bugfix at a time. Test your new functionality or bugfix using the included tests/test-vagrant.sh script to spin up a test VM. Run ansible-lint against the playbook before committing. (There is a VSCode task set up to run the right command for you) Ensure that your PR only changes files required for your functionality or bugfix. If you're adding a new application: Ensure that the new application is disabled by default. Add a documentation page to docs/applications/ - use an existing application as an example. Add the frontend port to docs/configuration/application_ports.md , ensuring you've not clashed with an existing application.","title":"Contributing"},{"location":"hardware/","text":"Hardware Ansible-NAS will run against any x64 Ubuntu box (i.e. not a Raspberry Pi or other ARM hardware), or even a VM if you're just testing things out. The hardware you pick for Ansible-NAS depends largely on what you intend to do with your server - and is very much a \"how long is a piece of string\" type question. The homeserver Reddit has lots of good information. ServerBuilds.net is also kept up to date with cheap and decent quality builds known as \"NAS Killers\". Questions To Think About What will be taxing the CPU(s) on your Ansible-NAS box? Are you intending to transcode video? How many users will be hitting it? How many applications do you intend to run in parallel? How much memory do these applications require? Are you going to run the ZFS file system? (you should!) What are you intending to store on your Ansible-NAS? Is it data you can download again, or is it important to you that you don't lose it? Do you need mirrored disks? HP Microserver Ansible-NAS development is tested against an HP Microserver N54L, with 16GB of memory, a 60gb SSD for the OS and 4x2TB WD Red NAS drives for storage. It works great :-) This is obviously not the only solution but a reasonable one if you just want a single box to buy, and many different models are available on eBay for varying costs.","title":"Hardware"},{"location":"hardware/#hardware","text":"Ansible-NAS will run against any x64 Ubuntu box (i.e. not a Raspberry Pi or other ARM hardware), or even a VM if you're just testing things out. The hardware you pick for Ansible-NAS depends largely on what you intend to do with your server - and is very much a \"how long is a piece of string\" type question. The homeserver Reddit has lots of good information. ServerBuilds.net is also kept up to date with cheap and decent quality builds known as \"NAS Killers\".","title":"Hardware"},{"location":"hardware/#questions-to-think-about","text":"What will be taxing the CPU(s) on your Ansible-NAS box? Are you intending to transcode video? How many users will be hitting it? How many applications do you intend to run in parallel? How much memory do these applications require? Are you going to run the ZFS file system? (you should!) What are you intending to store on your Ansible-NAS? Is it data you can download again, or is it important to you that you don't lose it? Do you need mirrored disks?","title":"Questions To Think About"},{"location":"hardware/#hp-microserver","text":"Ansible-NAS development is tested against an HP Microserver N54L, with 16GB of memory, a 60gb SSD for the OS and 4x2TB WD Red NAS drives for storage. It works great :-) This is obviously not the only solution but a reasonable one if you just want a single box to buy, and many different models are available on eBay for varying costs.","title":"HP Microserver"},{"location":"installation/","text":"You can run Ansible-NAS from the computer you plan to use for your NAS, or from a remote controlling machine. The steps for deployment are exactly the same, just pay attention to editing the inventory file in step 7. Enable the Ubuntu Universe repository: sudo add-apt-repository universe Install Ansible: sudo apt install ansible git clone https://github.com/davestephens/ansible-nas.git && cd ansible-nas Copy group_vars/all.yml.dist to group_vars/all.yml . Open up group_vars/all.yml and follow the instructions there for configuring your Ansible NAS. If you plan to use Transmission with OpenVPN, also copy group_vars/vpn_credentials.yml.dist to group_vars/vpn_credentials.yml and fill in your settings. Copy inventory.dist to inventory and update it. Install the dependent roles: ansible-galaxy install -r requirements.yml (you might need sudo to install Ansible roles) Run the playbook - something like ansible-playbook -i inventory nas.yml -b -K should do you nicely.","title":"Installation"},{"location":"overview/","text":"Ansible-NAS currently assumes you know your way around a server. This page is an overview for absolute NAS beginners so they can decide if it is right for them. The big picture To start off really simple: A NAS ( Network Attached Storage ) is a server mostly for home or other small networks that offers file storage. It's usually a small box that sits in the corner and runs 24/7. These days, a NAS doesn't just only handle files, but also offers other services, for instance video streaming with Plex or Emby . You can buy consumer NAS boxes from various manifacturers where you just have to add the hard drives, or you can configure your own hardware and use open-source software as the operating system. One example of the second variant you'll see mentioned here is FreeNAS . It is based on FreeBSD , which like Linux belongs to the family of Unix-like operating systems. One strength of FreeBSD/FreeNAS is that it includes the powerful ZFS file system ( OpenZFS , to be exact). However, it does not support the Docker containers the way Linux does. Also, the Linux ecosystem is larger. On the other hand, very few Linux distributions include ZFS out of the box because of licensing issues. Ansible-NAS in its default form attempts to have the best of both worlds by using Docker on Linux with ZFS. This is possible because the Ubuntu Linux distribution supports both technologies. As the name says, Ansible-NAS uses Ansible server automation which is usually deployed on big multi-machine enterprise systems, not small home servers the size of a breadbox. Before you take the plunge The commercial NAS vendors try to make setting up and running a NAS as simple and painless as possible - for a fee, obviously. The open-source NAS software providers have lots of resources to help you get started with your own hardware. FreeNAS for instance comes with extensive documentation, good introductions to ZFS and other topics, and a large community to lean on. With Ansible-NAS, at this point at least, you're pretty much on your own. Though there is a Gitter chat room (see support ), you're expected to have some familiarity with the technologies involved and be able to set up the basic stuff yourself. As a to-do list, before you can even install Ansible-NAS, you'll have to: Choose, buy, configure, and test your own hardware . If you're paranoid (a good mindset when dealing with servers), you'll probably want an uninterruptible power supply (UPS) of some sort as well as SMART monitoring for your hard drives. See the FreeNAS hardware requirements as a guideline, but remember you'll also be running Docker. If you use ZFS (see below), take into account it loves RAM and prefers to have the hard drives all to itself. Install Ubuntu Server , currently 18.04 LTS, and keep it updated. You'll probably want to perform other basic setup tasks like hardening SSH and including email notifications. There are various guides for this, but if you're just getting started, you'll probably need a book. You will probably want to install a specialized filesystem for bulk storage such as ZFS or Btrfs . Both offer features such as snapshots, checksumming and scrubing to protect your data against bitrot, ransomware and other nasties. Ansible-NAS historically prefers ZFS because this lets you swap storage pools with FreeNAS . A brief introduction to ZFS is included in the Ansible-NAS documentation, as well as an example of a very simple ZFS setup. After that, you can continue with the actual installation of Ansible-NAS. How to experiment The easiest way to take Ansible-NAS for a spin is in a virtual machine, for instance in VirtualBox . You'll want to create three virtual hard drives for testing: One of the actual NAS, and the two others to create a mirrored ZFS pool. This will let you experiment with installing, configuring, and running a complete system.","title":"Overview"},{"location":"overview/#the-big-picture","text":"To start off really simple: A NAS ( Network Attached Storage ) is a server mostly for home or other small networks that offers file storage. It's usually a small box that sits in the corner and runs 24/7. These days, a NAS doesn't just only handle files, but also offers other services, for instance video streaming with Plex or Emby . You can buy consumer NAS boxes from various manifacturers where you just have to add the hard drives, or you can configure your own hardware and use open-source software as the operating system. One example of the second variant you'll see mentioned here is FreeNAS . It is based on FreeBSD , which like Linux belongs to the family of Unix-like operating systems. One strength of FreeBSD/FreeNAS is that it includes the powerful ZFS file system ( OpenZFS , to be exact). However, it does not support the Docker containers the way Linux does. Also, the Linux ecosystem is larger. On the other hand, very few Linux distributions include ZFS out of the box because of licensing issues. Ansible-NAS in its default form attempts to have the best of both worlds by using Docker on Linux with ZFS. This is possible because the Ubuntu Linux distribution supports both technologies. As the name says, Ansible-NAS uses Ansible server automation which is usually deployed on big multi-machine enterprise systems, not small home servers the size of a breadbox.","title":"The big picture"},{"location":"overview/#before-you-take-the-plunge","text":"The commercial NAS vendors try to make setting up and running a NAS as simple and painless as possible - for a fee, obviously. The open-source NAS software providers have lots of resources to help you get started with your own hardware. FreeNAS for instance comes with extensive documentation, good introductions to ZFS and other topics, and a large community to lean on. With Ansible-NAS, at this point at least, you're pretty much on your own. Though there is a Gitter chat room (see support ), you're expected to have some familiarity with the technologies involved and be able to set up the basic stuff yourself. As a to-do list, before you can even install Ansible-NAS, you'll have to: Choose, buy, configure, and test your own hardware . If you're paranoid (a good mindset when dealing with servers), you'll probably want an uninterruptible power supply (UPS) of some sort as well as SMART monitoring for your hard drives. See the FreeNAS hardware requirements as a guideline, but remember you'll also be running Docker. If you use ZFS (see below), take into account it loves RAM and prefers to have the hard drives all to itself. Install Ubuntu Server , currently 18.04 LTS, and keep it updated. You'll probably want to perform other basic setup tasks like hardening SSH and including email notifications. There are various guides for this, but if you're just getting started, you'll probably need a book. You will probably want to install a specialized filesystem for bulk storage such as ZFS or Btrfs . Both offer features such as snapshots, checksumming and scrubing to protect your data against bitrot, ransomware and other nasties. Ansible-NAS historically prefers ZFS because this lets you swap storage pools with FreeNAS . A brief introduction to ZFS is included in the Ansible-NAS documentation, as well as an example of a very simple ZFS setup. After that, you can continue with the actual installation of Ansible-NAS.","title":"Before you take the plunge"},{"location":"overview/#how-to-experiment","text":"The easiest way to take Ansible-NAS for a spin is in a virtual machine, for instance in VirtualBox . You'll want to create three virtual hard drives for testing: One of the actual NAS, and the two others to create a mirrored ZFS pool. This will let you experiment with installing, configuring, and running a complete system.","title":"How to experiment"},{"location":"post_installation/","text":"So you've installed Ansible-NAS. Now what? The first thing to do is to configure Heimdall as the dashboard of your new NAS, because most of the applications included come with a web interface. Heimdall lets you create \"apps\" for them which appear as little icons on the screen. To add applications to Heimdall, you'll need the IP address of your NAS. If you don't know it for some reason, you will have to look up using the console with ip a . The entry \"link/ether\", usually the second one after the loopback device, will show the address. Another alternative is to make sure Avahi is installed for zero-configuration networking (mDNS). This will allow you to ssh into your NAS and with the extension .local to your machines name, such as ssh tardis.local . Then you can use the ip a command again. Next, you need the application's port, which you can look up in the list of ports . You can test the combination of address and port in your browser by typing them joined by a colon. For instance, for Glances on a machine with the IPv4 address 192.168.1.2, the full address would be http://192.168.1.2:61208 . Once you are sure it works, use this address and port combination when creating the Heimdall icon. Glances and Portainer are probably the two applications you want to add to Heimdall first, so you can see what is happening on the NAS. Note that Portainer will ask for your admin password. After that, it depends on what you have installed - see the listing for individual applications for more information.","title":"Post installation"},{"location":"support/","text":"Support Getting support for Ansible-NAS is easy! Gitter.im Ansible-NAS has its own Gitter chat room. davestephens hangs out there as well as a few existing users. Come say hi! GitHub Issues Raise an issue , using the supplied template to provide as much information as possible.","title":"Support"},{"location":"support/#support","text":"Getting support for Ansible-NAS is easy!","title":"Support"},{"location":"support/#gitterim","text":"Ansible-NAS has its own Gitter chat room. davestephens hangs out there as well as a few existing users. Come say hi!","title":"Gitter.im"},{"location":"support/#github-issues","text":"Raise an issue , using the supplied template to provide as much information as possible.","title":"GitHub Issues"},{"location":"testing/","text":"Vagrant A Vagrant Vagrantfile and launch script ( tests/test-vagrant.sh ) are provided to spin up a testing VM. The config in tests/test.yml is used by the script to override any existing config in group_vars/all.yml . By default the VM will be available on 172.30.1.5. If everything has worked correctly after running tests/test-vagrant.sh , you should be able to connect to Heimdall on http://172.30.1.5:10080. After making changes to the playbook, you can apply them to the running VM by running vagrant provision . Once you're done testing, destroy the VM with vagrant destroy . Travis CI Travis CI runs some sanity checks against branches once pushed to GitHub. These can be viewed here . ansible-lint ansible-lint is run as part of the CI (and VSCode tasks are provided) to ensure the playbook confirms to some sort of standard! You may or may not agree with all of the rules, but using it keeps things nice and consistent. Syntax Checking ansible-playbook --syntax-check is run against nas.yml to ensure nothing is majorly broken.","title":"Testing"},{"location":"testing/#vagrant","text":"A Vagrant Vagrantfile and launch script ( tests/test-vagrant.sh ) are provided to spin up a testing VM. The config in tests/test.yml is used by the script to override any existing config in group_vars/all.yml . By default the VM will be available on 172.30.1.5. If everything has worked correctly after running tests/test-vagrant.sh , you should be able to connect to Heimdall on http://172.30.1.5:10080. After making changes to the playbook, you can apply them to the running VM by running vagrant provision . Once you're done testing, destroy the VM with vagrant destroy .","title":"Vagrant"},{"location":"testing/#travis-ci","text":"Travis CI runs some sanity checks against branches once pushed to GitHub. These can be viewed here .","title":"Travis CI"},{"location":"testing/#ansible-lint","text":"ansible-lint is run as part of the CI (and VSCode tasks are provided) to ensure the playbook confirms to some sort of standard! You may or may not agree with all of the rules, but using it keeps things nice and consistent.","title":"ansible-lint"},{"location":"testing/#syntax-checking","text":"ansible-playbook --syntax-check is run against nas.yml to ensure nothing is majorly broken.","title":"Syntax Checking"},{"location":"upgrading/","text":"Upgrading Pull the latest Ansible-NAS repo, merge any new config sections from group_vars/all.yml.dist into your group_vars/all.yml , then run the playbook.","title":"Upgrading"},{"location":"upgrading/#upgrading","text":"Pull the latest Ansible-NAS repo, merge any new config sections from group_vars/all.yml.dist into your group_vars/all.yml , then run the playbook.","title":"Upgrading"},{"location":"applications/airsonic/","text":"Airsonic Homepage: https://airsonic.github.io/ Airsonic is a free, web-based media streamer, providing ubiquitous access to your music. Use it to share your music with friends, or to listen to your own music while at work. You can stream to multiple players simultaneously, for instance to one player in your kitchen and another in your living room Usage Set airsonic_enabled: true in your group_vars/all.yml file. The Airsonic web interface can be found at http://ansible_nas_host_or_ip:4040. Specific Configuration The default username and password is admin - you'll need to change this immediately after logging in.","title":"Airsonic"},{"location":"applications/airsonic/#airsonic","text":"Homepage: https://airsonic.github.io/ Airsonic is a free, web-based media streamer, providing ubiquitous access to your music. Use it to share your music with friends, or to listen to your own music while at work. You can stream to multiple players simultaneously, for instance to one player in your kitchen and another in your living room","title":"Airsonic"},{"location":"applications/airsonic/#usage","text":"Set airsonic_enabled: true in your group_vars/all.yml file. The Airsonic web interface can be found at http://ansible_nas_host_or_ip:4040.","title":"Usage"},{"location":"applications/airsonic/#specific-configuration","text":"The default username and password is admin - you'll need to change this immediately after logging in.","title":"Specific Configuration"},{"location":"applications/bitwarden/","text":"Bitwarden(_rs) Password Management Homepage: https://github.com/dani-garcia/bitwarden_rs Bitwarden: https://bitwarden.com/ This is a Bitwarden server API implementation written in Rust compatible with upstream Bitwarden clients*, perfect for self-hosted deployment where running the official resource-heavy service might not be ideal. Usage Set bitwarden_enabled: true in your group_vars/all.yml file. Specific Configuration Make sure you set your admin token! It is bitwarden_admin_token in group_vars/all.yml file. The string you put here will be the login to the admin section of your bitwarden installation (https://bitwarden.ansiblenasdomain.tld/admin). This token can be anything, but it's recommended to use a long, randomly generated string of characters, for example running: openssl rand -base64 48 . To create a user, you need to set bitwarden_allow_signups to true in your all.yml , and re-run the playbook to reprovision the container. Once you've created your users, set bitwarden_allow_signups back to false and run again. For speed you can target just Bitwarden by appending -t bitwarden to your ansible-playbook command.","title":"Bitwarden(_rs) Password Management"},{"location":"applications/bitwarden/#bitwarden_rs-password-management","text":"Homepage: https://github.com/dani-garcia/bitwarden_rs Bitwarden: https://bitwarden.com/ This is a Bitwarden server API implementation written in Rust compatible with upstream Bitwarden clients*, perfect for self-hosted deployment where running the official resource-heavy service might not be ideal.","title":"Bitwarden(_rs) Password Management"},{"location":"applications/bitwarden/#usage","text":"Set bitwarden_enabled: true in your group_vars/all.yml file.","title":"Usage"},{"location":"applications/bitwarden/#specific-configuration","text":"Make sure you set your admin token! It is bitwarden_admin_token in group_vars/all.yml file. The string you put here will be the login to the admin section of your bitwarden installation (https://bitwarden.ansiblenasdomain.tld/admin). This token can be anything, but it's recommended to use a long, randomly generated string of characters, for example running: openssl rand -base64 48 . To create a user, you need to set bitwarden_allow_signups to true in your all.yml , and re-run the playbook to reprovision the container. Once you've created your users, set bitwarden_allow_signups back to false and run again. For speed you can target just Bitwarden by appending -t bitwarden to your ansible-playbook command.","title":"Specific Configuration"},{"location":"applications/cloudflare_ddns/","text":"Cloudflare Dynamic DNS Updater Homepage: https://github.com/joshuaavalon/docker-cloudflare Cloudflare: https://www.cloudflare.com If you want your Ansible-NAS accessible externally then you'll need a domain name. You'll also need to set a wildcard host A record to point to your static IP, or enable this container to automatically update Cloudflare with your dynamic IP address. Usage Set cloudflare_ddns_enabled: true in your group_vars/all.yml file. Specific Configuration Make sure you set your Cloudflare login, domain and API key details within your group_vars/all.yml file.","title":"Cloudflare Dynamic DNS Updater"},{"location":"applications/cloudflare_ddns/#cloudflare-dynamic-dns-updater","text":"Homepage: https://github.com/joshuaavalon/docker-cloudflare Cloudflare: https://www.cloudflare.com If you want your Ansible-NAS accessible externally then you'll need a domain name. You'll also need to set a wildcard host A record to point to your static IP, or enable this container to automatically update Cloudflare with your dynamic IP address.","title":"Cloudflare Dynamic DNS Updater"},{"location":"applications/cloudflare_ddns/#usage","text":"Set cloudflare_ddns_enabled: true in your group_vars/all.yml file.","title":"Usage"},{"location":"applications/cloudflare_ddns/#specific-configuration","text":"Make sure you set your Cloudflare login, domain and API key details within your group_vars/all.yml file.","title":"Specific Configuration"},{"location":"applications/firefly/","text":"Firefly III Homepage: https://firefly-iii.org/ Firefly III is a self-hosted financial manager. It can help you keep track of expenses, income, budgets and everything in between. It supports credit cards, shared household accounts and savings accounts. It\u2019s pretty fancy. You should use it to save and organise money. Usage Set firefly_enabled: true in your group_vars/all.yml file. The very basic MiniDLNA web interface can be found at http://ansible_nas_host_or_ip:8066.","title":"Firefly III"},{"location":"applications/firefly/#firefly-iii","text":"Homepage: https://firefly-iii.org/ Firefly III is a self-hosted financial manager. It can help you keep track of expenses, income, budgets and everything in between. It supports credit cards, shared household accounts and savings accounts. It\u2019s pretty fancy. You should use it to save and organise money.","title":"Firefly III"},{"location":"applications/firefly/#usage","text":"Set firefly_enabled: true in your group_vars/all.yml file. The very basic MiniDLNA web interface can be found at http://ansible_nas_host_or_ip:8066.","title":"Usage"},{"location":"applications/get_iplayer/","text":"get_iplayer Homepage: https://github.com/get-iplayer/get_iplayer Downloads TV and radio programmes from BBC iPlayer. Usage Set get_iplayer_enabled: true in your group_vars/all.yml file. The get_iplayer web interface can be found at http://ansible_nas_host_or_ip:8182.","title":"get_iplayer"},{"location":"applications/get_iplayer/#get_iplayer","text":"Homepage: https://github.com/get-iplayer/get_iplayer Downloads TV and radio programmes from BBC iPlayer.","title":"get_iplayer"},{"location":"applications/get_iplayer/#usage","text":"Set get_iplayer_enabled: true in your group_vars/all.yml file. The get_iplayer web interface can be found at http://ansible_nas_host_or_ip:8182.","title":"Usage"},{"location":"applications/jackett/","text":"Jackett Homepage: https://github.com/Jackett/Jackett Jackett works as a proxy server: it translates queries from apps (Sonarr, Radarr, SickRage, CouchPotato, Mylar, DuckieTV, qBittorrent, Nefarious etc) into tracker-site-specific http queries, parses the html response, then sends results back to the requesting software. This allows for getting recent uploads (like RSS) and performing searches. Jackett is a single repository of maintained indexer scraping & translation logic - removing the burden from other apps. Usage Set jackett: true in your group_vars/all.yml file. The Jackett web interface can be found at http://ansible_nas_host_or_ip:9117.","title":"Jackett"},{"location":"applications/jackett/#jackett","text":"Homepage: https://github.com/Jackett/Jackett Jackett works as a proxy server: it translates queries from apps (Sonarr, Radarr, SickRage, CouchPotato, Mylar, DuckieTV, qBittorrent, Nefarious etc) into tracker-site-specific http queries, parses the html response, then sends results back to the requesting software. This allows for getting recent uploads (like RSS) and performing searches. Jackett is a single repository of maintained indexer scraping & translation logic - removing the burden from other apps.","title":"Jackett"},{"location":"applications/jackett/#usage","text":"Set jackett: true in your group_vars/all.yml file. The Jackett web interface can be found at http://ansible_nas_host_or_ip:9117.","title":"Usage"},{"location":"applications/minidlna/","text":"MiniDLNA Homepage: https://sourceforge.net/projects/minidlna/ MiniDLNA is server software with the aim of being fully compliant with DLNA/UPnP clients. The MiniDNLA daemon serves media files (music, pictures, and video) to clients on a network. Example clients include applications such as Totem and Kodi, and devices such as portable media players, Smartphones, Televisions, and gaming systems (such as PS3 and Xbox 360). Usage Set minidlna_enabled: true in your group_vars/all.yml file. The very basic MiniDLNA web interface can be found at http://ansible_nas_host_or_ip:8201.","title":"MiniDLNA"},{"location":"applications/minidlna/#minidlna","text":"Homepage: https://sourceforge.net/projects/minidlna/ MiniDLNA is server software with the aim of being fully compliant with DLNA/UPnP clients. The MiniDNLA daemon serves media files (music, pictures, and video) to clients on a network. Example clients include applications such as Totem and Kodi, and devices such as portable media players, Smartphones, Televisions, and gaming systems (such as PS3 and Xbox 360).","title":"MiniDLNA"},{"location":"applications/minidlna/#usage","text":"Set minidlna_enabled: true in your group_vars/all.yml file. The very basic MiniDLNA web interface can be found at http://ansible_nas_host_or_ip:8201.","title":"Usage"},{"location":"applications/miniflux/","text":"Miniflux Homepage: https://miniflux.app/ Miniflux is a minimalist and opinionated feed reader. Usage Set miniflux_enabled: true in your group_vars/all.yml file. The Miniflux web interface can be found at http://ansible_nas_host_or_ip:8070, the default username is admin and password supersecure . Specific Configuration An admin user will be created with the username and password of miniflux_admin_username and miniflux_admin_password respectively. These can be found in the Miniflux section within all.yml.dist .","title":"Miniflux"},{"location":"applications/miniflux/#miniflux","text":"Homepage: https://miniflux.app/ Miniflux is a minimalist and opinionated feed reader.","title":"Miniflux"},{"location":"applications/miniflux/#usage","text":"Set miniflux_enabled: true in your group_vars/all.yml file. The Miniflux web interface can be found at http://ansible_nas_host_or_ip:8070, the default username is admin and password supersecure .","title":"Usage"},{"location":"applications/miniflux/#specific-configuration","text":"An admin user will be created with the username and password of miniflux_admin_username and miniflux_admin_password respectively. These can be found in the Miniflux section within all.yml.dist .","title":"Specific Configuration"},{"location":"applications/mymediaforalexa/","text":"My Media for Alexa Homepage: https://www.mymediaalexa.com/](https://www.mymediaalexa.com/) My Media lets you stream your music collection to your Amazon Echo or Amazon Dot without having to upload all your music collection to the Cloud. This keeps your music under your control. Usage Set mymediaforalexa_enabled: true in your group_vars/all.yml file. The My Media for Alexa web interface can be found at http://ansible_nas_host_or_ip:52051.","title":"My Media for Alexa"},{"location":"applications/mymediaforalexa/#my-media-for-alexa","text":"Homepage: https://www.mymediaalexa.com/](https://www.mymediaalexa.com/) My Media lets you stream your music collection to your Amazon Echo or Amazon Dot without having to upload all your music collection to the Cloud. This keeps your music under your control.","title":"My Media for Alexa"},{"location":"applications/mymediaforalexa/#usage","text":"Set mymediaforalexa_enabled: true in your group_vars/all.yml file. The My Media for Alexa web interface can be found at http://ansible_nas_host_or_ip:52051.","title":"Usage"},{"location":"applications/nzbget/","text":"NZBget Homepage: https://nzbget.net/ The most efficient usenet downloader. NZBGet is written in C++ and designed with performance in mind to achieve maximum download speed by using very little system resources. Usage Set nzbget_enabled: true in your group_vars/all.yml file. The NZBget web interface can be found at http://ansible_nas_host_or_ip:6789, the default username is nzbget and password tegbzn6789 . Change this once you've logged in!","title":"NZBget"},{"location":"applications/nzbget/#nzbget","text":"Homepage: https://nzbget.net/ The most efficient usenet downloader. NZBGet is written in C++ and designed with performance in mind to achieve maximum download speed by using very little system resources.","title":"NZBget"},{"location":"applications/nzbget/#usage","text":"Set nzbget_enabled: true in your group_vars/all.yml file. The NZBget web interface can be found at http://ansible_nas_host_or_ip:6789, the default username is nzbget and password tegbzn6789 . Change this once you've logged in!","title":"Usage"},{"location":"applications/thelounge/","text":"The Lounge Homepage: https://thelounge.chat/ The Lounge is a self-hosted web IRC client. Usage Set thelounge_enabled: true in your group_vars/all.yml file. The Lounge web interface can be found at http://ansible_nas_host_or_ip:9000. Specific Configuration The default username and password is admin . Change this once you've logged in!","title":"The Lounge"},{"location":"applications/thelounge/#the-lounge","text":"Homepage: https://thelounge.chat/ The Lounge is a self-hosted web IRC client.","title":"The Lounge"},{"location":"applications/thelounge/#usage","text":"Set thelounge_enabled: true in your group_vars/all.yml file. The Lounge web interface can be found at http://ansible_nas_host_or_ip:9000.","title":"Usage"},{"location":"applications/thelounge/#specific-configuration","text":"The default username and password is admin . Change this once you've logged in!","title":"Specific Configuration"},{"location":"applications/timemachine/","text":"Time Machine Apple docs: https://support.apple.com/en-us/HT201250 Docker image: https://github.com/mbentley/docker-timemachine Time Machine is an application that allows you to backup files from your Mac. Usage Set timemachine_enabled: true in your group_vars/all.yml file. Specific Configuration timemachine_data_directory The absolute path on Ansible NAS where the backup files will be stored timemachine_volume_size_limit The maximum amount of space Time Machine can use for the backups in units of MiB. Set it to 0 for no limit. timemachine_share_name The name of the share as it will appear in the Time Machine application. Default is 'TimeMachine' timemachine_password The password used to access the share. Default is 'timemachine' timemachine_log_level The verbosity of the logs. 'Error' is the default.","title":"Time Machine"},{"location":"applications/timemachine/#time-machine","text":"Apple docs: https://support.apple.com/en-us/HT201250 Docker image: https://github.com/mbentley/docker-timemachine Time Machine is an application that allows you to backup files from your Mac.","title":"Time Machine"},{"location":"applications/timemachine/#usage","text":"Set timemachine_enabled: true in your group_vars/all.yml file.","title":"Usage"},{"location":"applications/timemachine/#specific-configuration","text":"timemachine_data_directory The absolute path on Ansible NAS where the backup files will be stored timemachine_volume_size_limit The maximum amount of space Time Machine can use for the backups in units of MiB. Set it to 0 for no limit. timemachine_share_name The name of the share as it will appear in the Time Machine application. Default is 'TimeMachine' timemachine_password The password used to access the share. Default is 'timemachine' timemachine_log_level The verbosity of the logs. 'Error' is the default.","title":"Specific Configuration"},{"location":"applications/traefik/","text":"Traefik Homepage: https://traefik.io Traefik is used to provide external access to your Ansible-NAS box. Additionally, Traefik will automatically request and renew SSL certificates for you. You can configure which applications are available externally by enabling the <application_name>_available_externally setting for each application in the Advanced Settings section of your all.yml . See External Access for more info. Usage Set traefik_enabled: true in your group_vars/all.yml file. Traefik's web interface can be found at http://ansible_nas_host_or_ip:8083. Specific Configuration You'll need to map port 80 and 443 from your router to your Ansible-NAS box. A quick search should reveal instruction for your model of router.","title":"Traefik"},{"location":"applications/traefik/#traefik","text":"Homepage: https://traefik.io Traefik is used to provide external access to your Ansible-NAS box. Additionally, Traefik will automatically request and renew SSL certificates for you. You can configure which applications are available externally by enabling the <application_name>_available_externally setting for each application in the Advanced Settings section of your all.yml . See External Access for more info.","title":"Traefik"},{"location":"applications/traefik/#usage","text":"Set traefik_enabled: true in your group_vars/all.yml file. Traefik's web interface can be found at http://ansible_nas_host_or_ip:8083.","title":"Usage"},{"location":"applications/traefik/#specific-configuration","text":"You'll need to map port 80 and 443 from your router to your Ansible-NAS box. A quick search should reveal instruction for your model of router.","title":"Specific Configuration"},{"location":"applications/transmission/","text":"Transmission Homepage: https://transmissionbt.com/ Transmission is a free BitTorrent client. Two versions are provided - one that tunnels through a VPN and one that connects directly. Usage Set transmission_enabled: true , or transmission_with_openvpn_enabled: true in your group_vars/all.yml file. Transmission's web interface can be found at http://ansible_nas_host_or_ip:9091 (with VPN) or http://ansible_nas_host_or_ip:9092 (without VPN). Specific Configuration If you enable Tranmission with OpenVPN, you'll need to copy group_vars/vpn_credentials.yml.dist to group_vars/vpn_credentials.yml and fill in your settings.","title":"Transmission"},{"location":"applications/transmission/#transmission","text":"Homepage: https://transmissionbt.com/ Transmission is a free BitTorrent client. Two versions are provided - one that tunnels through a VPN and one that connects directly.","title":"Transmission"},{"location":"applications/transmission/#usage","text":"Set transmission_enabled: true , or transmission_with_openvpn_enabled: true in your group_vars/all.yml file. Transmission's web interface can be found at http://ansible_nas_host_or_ip:9091 (with VPN) or http://ansible_nas_host_or_ip:9092 (without VPN).","title":"Usage"},{"location":"applications/transmission/#specific-configuration","text":"If you enable Tranmission with OpenVPN, you'll need to copy group_vars/vpn_credentials.yml.dist to group_vars/vpn_credentials.yml and fill in your settings.","title":"Specific Configuration"},{"location":"applications/watchtower/","text":"Watchtower Homepage: https://github.com/v2tec/watchtower A process for watching your Docker containers and automatically udpating and restarting them whenever their base image is refreshed. Usage Set watchtower_enabled: true in your group_vars/all.yml file. Specific Configuration By default Watchtower is configured to check daily at 5am for updates. Various notification options are available, and can be configured by updating watchtower_command in your group_vars/all.yml file. A few examples are provided. The full set of options can be found at the Watchtower GitHub project page .","title":"Watchtower"},{"location":"applications/watchtower/#watchtower","text":"Homepage: https://github.com/v2tec/watchtower A process for watching your Docker containers and automatically udpating and restarting them whenever their base image is refreshed.","title":"Watchtower"},{"location":"applications/watchtower/#usage","text":"Set watchtower_enabled: true in your group_vars/all.yml file.","title":"Usage"},{"location":"applications/watchtower/#specific-configuration","text":"By default Watchtower is configured to check daily at 5am for updates. Various notification options are available, and can be configured by updating watchtower_command in your group_vars/all.yml file. A few examples are provided. The full set of options can be found at the Watchtower GitHub project page .","title":"Specific Configuration"},{"location":"configuration/application_ports/","text":"Application Ports By default, applications can be found on the ports listed below. Application Port Notes Couchpotato 5050 Bitwarden \"hub\" 3012 Web Not. Bitwarden 19080 HTTP Duplicati 8200 Emby 8096 HTTP Emby 8920 HTTPS Firefly III 8066 get_iplayer 8182 Gitea 3001 Web Gitea 222 SSH Glances 61208 SSH Grafana 3000 Guacamole 8090 Heimdall 10080 Jackett 9117 MiniDLNA 8201 Miniflux 8070 MyMediaForAlexa 52051 Netdata 19999 Nextcloud 8080 NZBGet 6789 Plex 32400 Portainer 9000 Radarr 7878 Sickchill 8081 Sonarr 8989 Tautulli 8181 The Lounge 9000 Traefik 8083 Transmission 9091 with VPN Transmission 9092 ZNC 6677","title":"Application Ports"},{"location":"configuration/application_ports/#application-ports","text":"By default, applications can be found on the ports listed below. Application Port Notes Couchpotato 5050 Bitwarden \"hub\" 3012 Web Not. Bitwarden 19080 HTTP Duplicati 8200 Emby 8096 HTTP Emby 8920 HTTPS Firefly III 8066 get_iplayer 8182 Gitea 3001 Web Gitea 222 SSH Glances 61208 SSH Grafana 3000 Guacamole 8090 Heimdall 10080 Jackett 9117 MiniDLNA 8201 Miniflux 8070 MyMediaForAlexa 52051 Netdata 19999 Nextcloud 8080 NZBGet 6789 Plex 32400 Portainer 9000 Radarr 7878 Sickchill 8081 Sonarr 8989 Tautulli 8181 The Lounge 9000 Traefik 8083 Transmission 9091 with VPN Transmission 9092 ZNC 6677","title":"Application Ports"},{"location":"configuration/custom_applications/","text":"Custom Applications Using Portainer Ensure that you have portainer_enabled: true in your group_vars/all.yml file, and have run the playbook so that Portainer is up and running. Hit Portainer on http://ansible_nas_host_or_ip:9000. You can now deploy an 'App Template' or head to 'Containers' and manually enter container configuration. Using a Custom Ansible Task Needs to be docced","title":"Custom Applications"},{"location":"configuration/custom_applications/#custom-applications","text":"","title":"Custom Applications"},{"location":"configuration/custom_applications/#using-portainer","text":"Ensure that you have portainer_enabled: true in your group_vars/all.yml file, and have run the playbook so that Portainer is up and running. Hit Portainer on http://ansible_nas_host_or_ip:9000. You can now deploy an 'App Template' or head to 'Containers' and manually enter container configuration.","title":"Using Portainer"},{"location":"configuration/custom_applications/#using-a-custom-ansible-task","text":"Needs to be docced","title":"Using a Custom Ansible Task"},{"location":"configuration/external_access/","text":"External Access There are a number of steps required to enable external access to the applications running on your NAS: Enable Traefik Domain name and DNS configuration Router configuration Enable specific applications for external access :skull: :skull: :skull: Warning! :skull: :skull: :skull: Enabling access to applications externally does not automatically secure them. If you can access an application from within your own network without a username and password, this will also be the case externally. It is your responsiblity to ensure that applications you enable external access to are secured appropriately! Enable Traefik Traefik routes traffic from ports 80 (HTTP) and 443 (HTTPS) on your Ansible-NAS box to the relevant application, based on hostname. Simply set traefik_enabled: true in your all.yml . By default it listens on ports 80 and 443, but doesn't route any traffic. Domain Name and DNS Configuration Set ansible_nas_domain to the domain name you want to use for your Ansible-NAS. You'll need somewhere to host the DNS for that domain - Cloudflare is a good free solution. Once you have an account and Cloudflare is hosting the DNS for your domain, create a wildcard DNS entry ( *.myawesomedomain.com ) and set it to your current IP address. You then need to enable the Cloudflare Dynamic DNS container ( cloudflare_ddns_enabled: true ) so the wildcard DNS entry for your domain name is updated if/when your ISP issues you a new IP address. Router Configuration You need to map ports 80 and 443 from your router to your Ansible-NAS box. How to do this is entirely dependent on your router (and out of scope of these docs), but if you're using Ansible-NAS then this should be within your skillset. :) Enable Specific Applications Every application has a <application_name>_available_externally setting in the Advanced Settings section of all.yml . Setting this to true will configure Traefik to route <application>.yourdomain.com to the application, making it available externally.","title":"External Access"},{"location":"configuration/external_access/#external-access","text":"There are a number of steps required to enable external access to the applications running on your NAS: Enable Traefik Domain name and DNS configuration Router configuration Enable specific applications for external access","title":"External Access"},{"location":"configuration/external_access/#skull-skull-skull-warning-skull-skull-skull","text":"Enabling access to applications externally does not automatically secure them. If you can access an application from within your own network without a username and password, this will also be the case externally. It is your responsiblity to ensure that applications you enable external access to are secured appropriately!","title":":skull: :skull: :skull: Warning! :skull: :skull: :skull:"},{"location":"configuration/external_access/#enable-traefik","text":"Traefik routes traffic from ports 80 (HTTP) and 443 (HTTPS) on your Ansible-NAS box to the relevant application, based on hostname. Simply set traefik_enabled: true in your all.yml . By default it listens on ports 80 and 443, but doesn't route any traffic.","title":"Enable Traefik"},{"location":"configuration/external_access/#domain-name-and-dns-configuration","text":"Set ansible_nas_domain to the domain name you want to use for your Ansible-NAS. You'll need somewhere to host the DNS for that domain - Cloudflare is a good free solution. Once you have an account and Cloudflare is hosting the DNS for your domain, create a wildcard DNS entry ( *.myawesomedomain.com ) and set it to your current IP address. You then need to enable the Cloudflare Dynamic DNS container ( cloudflare_ddns_enabled: true ) so the wildcard DNS entry for your domain name is updated if/when your ISP issues you a new IP address.","title":"Domain Name and DNS Configuration"},{"location":"configuration/external_access/#router-configuration","text":"You need to map ports 80 and 443 from your router to your Ansible-NAS box. How to do this is entirely dependent on your router (and out of scope of these docs), but if you're using Ansible-NAS then this should be within your skillset. :)","title":"Router Configuration"},{"location":"configuration/external_access/#enable-specific-applications","text":"Every application has a <application_name>_available_externally setting in the Advanced Settings section of all.yml . Setting this to true will configure Traefik to route <application>.yourdomain.com to the application, making it available externally.","title":"Enable Specific Applications"},{"location":"configuration/nfs_exports/","text":"NFS Exports Ansible-NAS uses the awesome geerlingguy.nfs Ansible role to configure NFS exports. More info on configuring NFS exports can be found here . NFS Examples Ansible-NAS shares are defined in the nfs_exports section within group_vars/all.yml . The example provided will allow anyone to read the data in {{ nfs_shares_root }}/public on your Ansible-NAS box. Permissions NFS \"exports\" (an equivalent of a Samba share) are permissioned differently to Samba shares. Samba shares are permissioned with users and groups, and NFS exports are permissioned by the host wanting to access them, and then usual Linux permissions are applied to the files and directories within there. As mentioned above, the example will allow any computer on your network to read and write to the export.","title":"NFS Exports"},{"location":"configuration/nfs_exports/#nfs-exports","text":"Ansible-NAS uses the awesome geerlingguy.nfs Ansible role to configure NFS exports. More info on configuring NFS exports can be found here .","title":"NFS Exports"},{"location":"configuration/nfs_exports/#nfs-examples","text":"Ansible-NAS shares are defined in the nfs_exports section within group_vars/all.yml . The example provided will allow anyone to read the data in {{ nfs_shares_root }}/public on your Ansible-NAS box.","title":"NFS Examples"},{"location":"configuration/nfs_exports/#permissions","text":"NFS \"exports\" (an equivalent of a Samba share) are permissioned differently to Samba shares. Samba shares are permissioned with users and groups, and NFS exports are permissioned by the host wanting to access them, and then usual Linux permissions are applied to the files and directories within there. As mentioned above, the example will allow any computer on your network to read and write to the export.","title":"Permissions"},{"location":"configuration/samba_shares/","text":"Samba Shares Ansible-NAS uses the awesome bertvv.samba Ansible role to configure Samba - check out the project page for the many different options you can use to configure a share. Share Examples Ansible-NAS shares are defined in the samba_shares section within group_vars/all.yml . The examples provided are \"public\" shares that anyone on your LAN can read and write to. File Permissions Ansible-NAS creates an ansible-nas user and group on your server, which Samba will use to access the data in your shares. New data created will be permissioned correctly. However, if you have existing data this will need to be repermissioned so that Samba can read and serve it. An playbook is provided to do this for you - permission_data.yml . It is separated from the main Ansible-NAS playbook due to the time it can take to run with large amounts of data. You should only need to run this once.","title":"Samba Shares"},{"location":"configuration/samba_shares/#samba-shares","text":"Ansible-NAS uses the awesome bertvv.samba Ansible role to configure Samba - check out the project page for the many different options you can use to configure a share.","title":"Samba Shares"},{"location":"configuration/samba_shares/#share-examples","text":"Ansible-NAS shares are defined in the samba_shares section within group_vars/all.yml . The examples provided are \"public\" shares that anyone on your LAN can read and write to.","title":"Share Examples"},{"location":"configuration/samba_shares/#file-permissions","text":"Ansible-NAS creates an ansible-nas user and group on your server, which Samba will use to access the data in your shares. New data created will be permissioned correctly. However, if you have existing data this will need to be repermissioned so that Samba can read and serve it. An playbook is provided to do this for you - permission_data.yml . It is separated from the main Ansible-NAS playbook due to the time it can take to run with large amounts of data. You should only need to run this once.","title":"File Permissions"},{"location":"zfs/zfs_configuration/","text":"ZFS Configuration This text deals with specific ZFS configuration questions for Ansible-NAS. If you are new to ZFS and are looking for the big picture, please read the ZFS overview introduction first. Just so there is no misunderstanding Unlike other NAS variants, Ansible-NAS does not install, configure or manage the disks or file systems for you. It doesn't care which file system you use - ZFS, Btrfs, XFS or EXT4, take your pick. Nor does it provides a mechanism for snapshots or disk monitoring. As Tony Stark said to Loki in Avengers : It's all on you. However, Ansible-NAS has traditionally been used with the powerful ZFS filesystem. Since out of the box support for ZFS on Linux with Ubuntu is comparatively new, this text shows how to set up a simple storage configuration. To paraphrase Nick Fury from Winter Soldier : We do share. We're nice like that. Using ZFS for Docker containers is currently not covered by this document. See the official Docker ZFS documentation instead. The obligatory warning We take no responsibility for any bad thing that might happen if you follow this guide. We strongly suggest you test these procedures in a virtual machine first. Always, always, always backup your data. The basic setup For this example, we're assuming two identical spinning rust hard drives for Ansible-NAS storage. These two drives will be mirrored to provide redundancy. The actual Ubuntu system will be on a different drive and is not our concern. Root on ZFS is still a hassle for Ubuntu. If that changes, this document might be updated accordingly. Until then, don't ask us about it. The Ubuntu kernel is already ready for ZFS. We only need the utility package which we install with sudo apt install zfsutils . Creating a pool We assume you don't mind totally destroying whatever data might be on your two storage drives, have used a tool such as gparted to remove any existing partitions, and have installed a new GPT partition table on each drive. To create our ZFS pool, we will use a command in this form: sudo zpool create -o ashift=<ASHIFT> <NAME> mirror <DRIVE1> <DRIVE2> The options from simple to complex are: NAME : ZFS pools traditionally take their names from characters in the The Matrix . The two most common are tank and dozer . Whatever you use, it should be short - think ash , not xenomorph . DRIVES : The Linux command lsblk will give you a quick overview of the hard drives in the system. However, we don't pass the drive specification in the format /dev/sde because this is not persistent. Instead, always use the output of ls /dev/disk/by-id/ to find the drives' IDs. ASHIFT : This is required to pass the sector size of the drive to ZFS for optimal performance. You might have to do this by hand because some drives lie: Whereas modern drives have 4k sector sizes (or 8k for many SSDs), they will report 512 bytes because Windows XP can't handle 4k sectors . ZFS tries to catch the liars and use the correct value. However, this sometimes fails, and you have to add it by hand. The ashift value is a power of two, so we have 9 for 512 bytes, 12 for 4k, and 13 for 8k. You can create a pool without this parameter and then use zdb -C | grep ashift to see what ZFS generated automatically. If it isn't what you think, destroy the pool again and add it manually. In our pretend case, we use two 3 TB WD Red drives. Listing all drives by ID gives us something like this, but with real serial numbers: ata-WDC_WD30EFRX-68EUZN0_WD-WCCFAKESN01 ata-WDC_WD30EFRX-68EUZN0_WD-WCCFAKESN02 WD Reds have a 4k sector size. The actual command to create the pool would then be: sudo zpool create -o ashift=12 tank mirror ata-WDC_WD30EFRX-68EUZN0_WD-WCCFAKESN01 ata-WDC_WD30EFRX-68EUZN0_WD-WCCFAKESN02 Our new pool is named tank and is mirrored. To see information about it, use zpool status tank (no sudo necessary). If you screwed up (usually with ashift ), use sudo zpool destroy tank and start over now before it's too late. Pool default parameters Setting pool-wide default parameters makes life easier when we create our filesystems. To see them all, you can use the command zfs get all tank . Most are perfectly sensible, some you'll want to change : sudo zfs set atime=off tank sudo zfs set compression=lz4 tank sudo zfs set autoexpand=on tank The atime parameter means that your system updates a time stamp every time a file is accessed, which uses a lot of resources. Usually, you don't care. Compression is a no-brainer on modern CPUs and should be on by default (we will discuss exceptions for compressed media files later). The autoexpand lets the pool grow when you add larger hard drives. Creating filesystems To actually store the data, we need filesystems (also known as \"datasets\"). For our very simple default Ansible-NAS setup, we will create two: One filesystem for movies ( movies_root in all.yml ) and one for downloads ( downloads_root ). Movies (and other large, pre-compressed files) We first create the basic filesystem: sudo zfs create tank/movies Movie files are usually rather large, already in a compressed format and for security reasons, the files stored there shouldn't be executable. We change the properties of the filesystem accordingly: sudo zfs set recordsize=1M tank/movies sudo zfs set compression=off tank/movies sudo zfs set exec=off tank/movies The recordsize here is set to the currently largest possible value to increase performance and save storage. Recall that we used ashift during the creation of the pool to match the ZFS block size with the drives' sector size. Records are created out of these blocks. Having larger records reduces the amount of metadata that is required, because various parts of ZFS such as caching and checksums work on this level. Compression is unnecessary for movie files because they are usually in a compressed format anyway. ZFS is good about recognizing this, and so if you happen to leave compression on as the default for the pool, it won't make much of a difference. By default , ZFS stores pools directly under the root directory. Also, the filesystems don't have to be listed in /etc/fstab to be mounted. This means that our filesystem will appear as /tank/movies if you don't change anything. We need to change the line in all.yml accordingly: movies_root: \"/tank/movies\" You can also set a traditional mount point if you wish with the mountpoint property. Setting this to none prevents the file system from being automatically mounted at all. The filesystems for TV shows, music files and podcasts - all large, pre-compressed files - should probably take the exact same parameters. Downloads For downloads, we can leave most of the default parameters the way they are. sudo zfs create tank/downloads sudo zfs set exec=off tank/downloads The recordsize stays the 128 KB default. In all.yml , the new line is downloads_root: \"/tank/downloads\" Other data Depending on the use case, you might want to create and tune more filesystems. For example, Bit Torrent , MySQL and Virtual Machines all have known best configurations. Setting up scrubs On Ubuntu, scrubs are configured out of the box to run on the second Sunday of every month. See /etc/cron.d/zfsutils-linux to change this. Email notifications To have the ZFS demon zed send you emails when there is trouble, you first have to install an email agent such as postfix. In the file /etc/zfs/zed.d/zed.rc , change the three entries: ZED_EMAIL_ADDR=<YOUR_EMAIL_ADDRESS_HERE> ZED_NOTIFY_INTERVAL_SECS=3600 ZED_NOTIFY_VERBOSE=1 If zed is not enabled, you might have to run systemctl enable zed . You can test the setup by manually starting a scrub with sudo zpool scrub tank . Snapshots Snapshots create a \"frozen\" version of a filesystem, providing a safe copy of the contents. Correctly configured, they provide good protection against accidental deletion and certain types of attacks such as ransomware. On copy-on-write (COW) filesystems such as ZFS, they are cheap and fast to create. It is very rare that you won't want snapshots. Snapshots do not replace the need for backups. Nothing replaces the need for backups except more backups. Managing snapshots by hand If you have data in a filesystem that never or very rarely changes, it might be easiest to just take a snapshot by hand after every major change. Use the zfs snapshot command with the name of the filesystem combined with an identifier separated by the @ sign. Traditionally, this somehow includes the date of the snapshot, usually in some variant of the ISO 8601 format. zfs snapshot tank/movies@2019-04-24 To see the list of snapshots in the system, run zfs list -t snapshot To revert (\"roll back\") to the previous snapshot, use the zfs rollback command. zfs rollback tank/movies@2019-04-24 By default, you can only roll back to the most recent snapshot. Anything before then requires trickery outside the scope of this document. Finally, to get rid of a snapshot, use the zfs destroy command. zfs destroy tank/movies@2019-04-24 Be very careful with destroy . If you leave out the snapshot identifier and only list the filesystem - in our example, tank/movies - the filesystem itself will immediately be destroyed. There will be no confirmation prompt, because ZFS doesn't believe in that sort of thing. Managing snapshots with Sanoid Usually, you'll want the process of creating new and deleting old snapshots to be automatic, especially on filesystems that change frequently. One tool for this is sanoid . There are various instructions for setting it up, the following is based on notes from SvennD . For this example, we'll assume we have a single dataset tank/movies that holds, ah, movies. First, we install sanoid to the /opt directory. This assumes that Perl itself is already installed. sudo apt install libconfig-inifiles-perl cd /opt sudo git clone https://github.com/jimsalterjrs/sanoid It is probably easiest to link sanoid to /usr/sbin : sudo ln /opt/sanoid/sanoid /usr/sbin/ Then we need to setup the configuration files. sudo mkdir /etc/sanoid sudo cp /opt/sanoid/sanoid.conf /etc/sanoid/sanoid.conf sudo cp /opt/sanoid/sanoid.defaults.conf /etc/sanoid/sanoid.defaults.conf We don't change the defaults file, but it has to be copied to the folder anyway. Next, we edit the /etc/sanoid/sanoid.conf configuration file in two steps: We design the \"templates\" and then tell sanoid which filesystems to use it on. The configuration file included with sanoid contains a \"production\" template for filesystems that change frequently. For media files, we assume that there is not going to be that much change from day-to-day, and especially there will be very few deletions. We use snapshots because this provides protection against cryptolocker attacks and against accidental deletions. Again, snapshots, even lots of snapshots, do not replace backups. For our example, we configure for two hourly snapshots (against \"oh crap\" deletions), 31 daily, one monthly and one yearly snapshot. [template_media] frequently = 0 hourly = 2 daily = 31 monthly = 1 yearly = 1 autosnap = yes autoprune = yes That might seem like a bunch of daily snapshots, but remember, if nothing has changed, a ZFS snapshot is basically free. Once we have an entry for the template, we assign it to the filesystem. [tank/movies] use_template = media Finally, we edit /etc/crontab to run sanoid every five minutes: */5 * * * * root /usr/sbin/sanoid --cron After five minutes, you should see the first snapshots (use zfs list -t snapshot again). The list will look something like this mock example: NAME USED AVAIL REFER MOUNTPOINT tank/movies@autosnap_2019-05-17_13:55:01_yearly 0B - 1,53G - tank/movies@autosnap_2019-05-17_13:55:01_monthly 0B - 1,53G - tank/movies@autosnap_2019-05-17_13:55:01_daily 0B - 1,53G - Note that the snapshots use no storage, because we haven't changed anything. This is a very simple use of sanoid. Other functions include running scripts before and after snapshots, and setups to help with backups. See the included configuration files for examples.","title":"ZFS Configuration"},{"location":"zfs/zfs_configuration/#zfs-configuration","text":"This text deals with specific ZFS configuration questions for Ansible-NAS. If you are new to ZFS and are looking for the big picture, please read the ZFS overview introduction first.","title":"ZFS Configuration"},{"location":"zfs/zfs_configuration/#just-so-there-is-no-misunderstanding","text":"Unlike other NAS variants, Ansible-NAS does not install, configure or manage the disks or file systems for you. It doesn't care which file system you use - ZFS, Btrfs, XFS or EXT4, take your pick. Nor does it provides a mechanism for snapshots or disk monitoring. As Tony Stark said to Loki in Avengers : It's all on you. However, Ansible-NAS has traditionally been used with the powerful ZFS filesystem. Since out of the box support for ZFS on Linux with Ubuntu is comparatively new, this text shows how to set up a simple storage configuration. To paraphrase Nick Fury from Winter Soldier : We do share. We're nice like that. Using ZFS for Docker containers is currently not covered by this document. See the official Docker ZFS documentation instead.","title":"Just so there is no misunderstanding"},{"location":"zfs/zfs_configuration/#the-obligatory-warning","text":"We take no responsibility for any bad thing that might happen if you follow this guide. We strongly suggest you test these procedures in a virtual machine first. Always, always, always backup your data.","title":"The obligatory warning"},{"location":"zfs/zfs_configuration/#the-basic-setup","text":"For this example, we're assuming two identical spinning rust hard drives for Ansible-NAS storage. These two drives will be mirrored to provide redundancy. The actual Ubuntu system will be on a different drive and is not our concern. Root on ZFS is still a hassle for Ubuntu. If that changes, this document might be updated accordingly. Until then, don't ask us about it. The Ubuntu kernel is already ready for ZFS. We only need the utility package which we install with sudo apt install zfsutils .","title":"The basic setup"},{"location":"zfs/zfs_configuration/#creating-a-pool","text":"We assume you don't mind totally destroying whatever data might be on your two storage drives, have used a tool such as gparted to remove any existing partitions, and have installed a new GPT partition table on each drive. To create our ZFS pool, we will use a command in this form: sudo zpool create -o ashift=<ASHIFT> <NAME> mirror <DRIVE1> <DRIVE2> The options from simple to complex are: NAME : ZFS pools traditionally take their names from characters in the The Matrix . The two most common are tank and dozer . Whatever you use, it should be short - think ash , not xenomorph . DRIVES : The Linux command lsblk will give you a quick overview of the hard drives in the system. However, we don't pass the drive specification in the format /dev/sde because this is not persistent. Instead, always use the output of ls /dev/disk/by-id/ to find the drives' IDs. ASHIFT : This is required to pass the sector size of the drive to ZFS for optimal performance. You might have to do this by hand because some drives lie: Whereas modern drives have 4k sector sizes (or 8k for many SSDs), they will report 512 bytes because Windows XP can't handle 4k sectors . ZFS tries to catch the liars and use the correct value. However, this sometimes fails, and you have to add it by hand. The ashift value is a power of two, so we have 9 for 512 bytes, 12 for 4k, and 13 for 8k. You can create a pool without this parameter and then use zdb -C | grep ashift to see what ZFS generated automatically. If it isn't what you think, destroy the pool again and add it manually. In our pretend case, we use two 3 TB WD Red drives. Listing all drives by ID gives us something like this, but with real serial numbers: ata-WDC_WD30EFRX-68EUZN0_WD-WCCFAKESN01 ata-WDC_WD30EFRX-68EUZN0_WD-WCCFAKESN02 WD Reds have a 4k sector size. The actual command to create the pool would then be: sudo zpool create -o ashift=12 tank mirror ata-WDC_WD30EFRX-68EUZN0_WD-WCCFAKESN01 ata-WDC_WD30EFRX-68EUZN0_WD-WCCFAKESN02 Our new pool is named tank and is mirrored. To see information about it, use zpool status tank (no sudo necessary). If you screwed up (usually with ashift ), use sudo zpool destroy tank and start over now before it's too late.","title":"Creating a pool"},{"location":"zfs/zfs_configuration/#pool-default-parameters","text":"Setting pool-wide default parameters makes life easier when we create our filesystems. To see them all, you can use the command zfs get all tank . Most are perfectly sensible, some you'll want to change : sudo zfs set atime=off tank sudo zfs set compression=lz4 tank sudo zfs set autoexpand=on tank The atime parameter means that your system updates a time stamp every time a file is accessed, which uses a lot of resources. Usually, you don't care. Compression is a no-brainer on modern CPUs and should be on by default (we will discuss exceptions for compressed media files later). The autoexpand lets the pool grow when you add larger hard drives.","title":"Pool default parameters"},{"location":"zfs/zfs_configuration/#creating-filesystems","text":"To actually store the data, we need filesystems (also known as \"datasets\"). For our very simple default Ansible-NAS setup, we will create two: One filesystem for movies ( movies_root in all.yml ) and one for downloads ( downloads_root ).","title":"Creating filesystems"},{"location":"zfs/zfs_configuration/#movies-and-other-large-pre-compressed-files","text":"We first create the basic filesystem: sudo zfs create tank/movies Movie files are usually rather large, already in a compressed format and for security reasons, the files stored there shouldn't be executable. We change the properties of the filesystem accordingly: sudo zfs set recordsize=1M tank/movies sudo zfs set compression=off tank/movies sudo zfs set exec=off tank/movies The recordsize here is set to the currently largest possible value to increase performance and save storage. Recall that we used ashift during the creation of the pool to match the ZFS block size with the drives' sector size. Records are created out of these blocks. Having larger records reduces the amount of metadata that is required, because various parts of ZFS such as caching and checksums work on this level. Compression is unnecessary for movie files because they are usually in a compressed format anyway. ZFS is good about recognizing this, and so if you happen to leave compression on as the default for the pool, it won't make much of a difference. By default , ZFS stores pools directly under the root directory. Also, the filesystems don't have to be listed in /etc/fstab to be mounted. This means that our filesystem will appear as /tank/movies if you don't change anything. We need to change the line in all.yml accordingly: movies_root: \"/tank/movies\" You can also set a traditional mount point if you wish with the mountpoint property. Setting this to none prevents the file system from being automatically mounted at all. The filesystems for TV shows, music files and podcasts - all large, pre-compressed files - should probably take the exact same parameters.","title":"Movies (and other large, pre-compressed files)"},{"location":"zfs/zfs_configuration/#downloads","text":"For downloads, we can leave most of the default parameters the way they are. sudo zfs create tank/downloads sudo zfs set exec=off tank/downloads The recordsize stays the 128 KB default. In all.yml , the new line is downloads_root: \"/tank/downloads\"","title":"Downloads"},{"location":"zfs/zfs_configuration/#other-data","text":"Depending on the use case, you might want to create and tune more filesystems. For example, Bit Torrent , MySQL and Virtual Machines all have known best configurations.","title":"Other data"},{"location":"zfs/zfs_configuration/#setting-up-scrubs","text":"On Ubuntu, scrubs are configured out of the box to run on the second Sunday of every month. See /etc/cron.d/zfsutils-linux to change this.","title":"Setting up scrubs"},{"location":"zfs/zfs_configuration/#email-notifications","text":"To have the ZFS demon zed send you emails when there is trouble, you first have to install an email agent such as postfix. In the file /etc/zfs/zed.d/zed.rc , change the three entries: ZED_EMAIL_ADDR=<YOUR_EMAIL_ADDRESS_HERE> ZED_NOTIFY_INTERVAL_SECS=3600 ZED_NOTIFY_VERBOSE=1 If zed is not enabled, you might have to run systemctl enable zed . You can test the setup by manually starting a scrub with sudo zpool scrub tank .","title":"Email notifications"},{"location":"zfs/zfs_configuration/#snapshots","text":"Snapshots create a \"frozen\" version of a filesystem, providing a safe copy of the contents. Correctly configured, they provide good protection against accidental deletion and certain types of attacks such as ransomware. On copy-on-write (COW) filesystems such as ZFS, they are cheap and fast to create. It is very rare that you won't want snapshots. Snapshots do not replace the need for backups. Nothing replaces the need for backups except more backups.","title":"Snapshots"},{"location":"zfs/zfs_configuration/#managing-snapshots-by-hand","text":"If you have data in a filesystem that never or very rarely changes, it might be easiest to just take a snapshot by hand after every major change. Use the zfs snapshot command with the name of the filesystem combined with an identifier separated by the @ sign. Traditionally, this somehow includes the date of the snapshot, usually in some variant of the ISO 8601 format. zfs snapshot tank/movies@2019-04-24 To see the list of snapshots in the system, run zfs list -t snapshot To revert (\"roll back\") to the previous snapshot, use the zfs rollback command. zfs rollback tank/movies@2019-04-24 By default, you can only roll back to the most recent snapshot. Anything before then requires trickery outside the scope of this document. Finally, to get rid of a snapshot, use the zfs destroy command. zfs destroy tank/movies@2019-04-24 Be very careful with destroy . If you leave out the snapshot identifier and only list the filesystem - in our example, tank/movies - the filesystem itself will immediately be destroyed. There will be no confirmation prompt, because ZFS doesn't believe in that sort of thing.","title":"Managing snapshots by hand"},{"location":"zfs/zfs_configuration/#managing-snapshots-with-sanoid","text":"Usually, you'll want the process of creating new and deleting old snapshots to be automatic, especially on filesystems that change frequently. One tool for this is sanoid . There are various instructions for setting it up, the following is based on notes from SvennD . For this example, we'll assume we have a single dataset tank/movies that holds, ah, movies. First, we install sanoid to the /opt directory. This assumes that Perl itself is already installed. sudo apt install libconfig-inifiles-perl cd /opt sudo git clone https://github.com/jimsalterjrs/sanoid It is probably easiest to link sanoid to /usr/sbin : sudo ln /opt/sanoid/sanoid /usr/sbin/ Then we need to setup the configuration files. sudo mkdir /etc/sanoid sudo cp /opt/sanoid/sanoid.conf /etc/sanoid/sanoid.conf sudo cp /opt/sanoid/sanoid.defaults.conf /etc/sanoid/sanoid.defaults.conf We don't change the defaults file, but it has to be copied to the folder anyway. Next, we edit the /etc/sanoid/sanoid.conf configuration file in two steps: We design the \"templates\" and then tell sanoid which filesystems to use it on. The configuration file included with sanoid contains a \"production\" template for filesystems that change frequently. For media files, we assume that there is not going to be that much change from day-to-day, and especially there will be very few deletions. We use snapshots because this provides protection against cryptolocker attacks and against accidental deletions. Again, snapshots, even lots of snapshots, do not replace backups. For our example, we configure for two hourly snapshots (against \"oh crap\" deletions), 31 daily, one monthly and one yearly snapshot. [template_media] frequently = 0 hourly = 2 daily = 31 monthly = 1 yearly = 1 autosnap = yes autoprune = yes That might seem like a bunch of daily snapshots, but remember, if nothing has changed, a ZFS snapshot is basically free. Once we have an entry for the template, we assign it to the filesystem. [tank/movies] use_template = media Finally, we edit /etc/crontab to run sanoid every five minutes: */5 * * * * root /usr/sbin/sanoid --cron After five minutes, you should see the first snapshots (use zfs list -t snapshot again). The list will look something like this mock example: NAME USED AVAIL REFER MOUNTPOINT tank/movies@autosnap_2019-05-17_13:55:01_yearly 0B - 1,53G - tank/movies@autosnap_2019-05-17_13:55:01_monthly 0B - 1,53G - tank/movies@autosnap_2019-05-17_13:55:01_daily 0B - 1,53G - Note that the snapshots use no storage, because we haven't changed anything. This is a very simple use of sanoid. Other functions include running scripts before and after snapshots, and setups to help with backups. See the included configuration files for examples.","title":"Managing snapshots with Sanoid"},{"location":"zfs/zfs_overview/","text":"ZFS Overview This is a general overview of the ZFS file system for people who are new to it. If you have some experience and are actually looking for specific information about how to configure ZFS for Ansible-NAS, check out the ZFS example configuration . What is ZFS and why would I want it? ZFS is an advanced filesystem and volume manager originally created by Sun Microsystems starting in 2001. First released in 2005 for OpenSolaris, Oracle later bought Sun and switched to developing ZFS as closed source software. An open source fork took the name OpenZFS , but is still called \"ZFS\" for short. It runs on Linux, FreeBSD, illumos and other platforms. ZFS aims to be the \"last word in filesystems\" , a technology so future-proof that Michael W. Lucas and Allan Jude famously stated that the Enterprise's computer on Star Trek probably runs it. The design was based on four principles : \"Pooled\" storage to eliminate the notion of volumes. You can add more storage the same way you just add a RAM stick to memory. Make sure data is always consistent on the disks. There is no fsck command for ZFS and none is needed. Detect and correct data corruption (\"bitrot\"). ZFS is one of the few storage systems that checksums everything, including the data itself, and is \"self-healing\". Make it easy to use. Try to \"end the suffering\" for the admins involved in managing storage. ZFS includes a host of other features such as snapshots, transparent compression and encryption. During the early years of ZFS, this all came with hardware requirements only enterprise users could afford. By now, however, computers have become so powerful that ZFS can run (with some effort) on a Raspberry Pi . FreeBSD and FreeNAS make extensive use of ZFS. What is holding ZFS back on Linux are licensing issues beyond the scope of this document. Ansible-NAS doesn't actually specify a filesystem - you can use EXT4, XFS or Btrfs as well. However, ZFS not only provides the benefits listed above, but also lets you use your hard drives with different operating systems. Some people now using Ansible-NAS came from FreeNAS, and were able to export their ZFS storage drives there and import them to Ubuntu. On the other hand, if you ever decide to switch back to FreeNAS or maybe want to use FreeBSD instead of Linux, you should be able to use the same ZFS pools. An overview and some actual commands Storage in ZFS is organized in pools . Inside these pools, you create filesystems (also known as \"datasets\") which are like partitions on steroids. For instance, you can keep each user's /home directory in a separate filesystem. ZFS systems tend to use lots and lots of specialized filesystems with tailored parameters such as record size and compression. All filesystems share the available storage in their pool. Pools do not directly consist of hard disks or SSDs. Instead, drives are organized as virtual devices (VDEVs). This is where the physical redundancy in ZFS is located. Drives in a VDEV can be \"mirrored\" or combined as \"RaidZ\", roughly the equivalent of RAID5. These VDEVs are then combined into a pool by the administrator. The command might look something like this: sudo zpool create tank mirror /dev/sda /dev/sdb This combines /dev/sba and /dev/sdb to a mirrored VDEV, and then defines a new pool named tank consisting of this single VDEV. (Actually, you'd want to use a different ID for the drives, but you get the idea.) You can now create a filesystem in this pool for, say, all of your Mass Effect fan fiction: sudo zfs create tank/mefanfic You can then enable automatic compression on this filesystem with sudo zfs set compression=lz4 tank/mefanfic . To take a snapshot , use sudo zfs snapshot tank/mefanfic@21540411 Now, if evil people were somehow able to encrypt your precious fan fiction files with ransomware, you can simply laugh maniacally and revert to the old version: sudo zfs rollback tank/mefanfic@21540411 Of course, you would lose any texts you might have added to the filesystem between that snapshot and now. Usually, you'll have some form of automatic snapshot administration configured. To detect bitrot and other data defects, ZFS periodically runs scrubs : The system compares the available copies of each data record with their checksums. If there is a mismatch, the data is repaired. Known issues At time of writing (April 2019), ZFS on Linux does not offer native encryption, TRIM support or device removal, which are all scheduled to be included in the upcoming 0.8 release any day now. ZFS' original design for enterprise systems and redundancy requirements can make some things difficult. You can't just add individual drives to a pool and tell the system to reconfigure automatically. Instead, you have to either add a new VDEV, or replace each of the existing drives with one of higher capacity. In an enterprise environment, of course, you would just buy a bunch of new drives and move the data from the old pool to the new pool. Shrinking a pool is even harder - put simply, ZFS is not built for this, though it is being worked on . If you absolutely must be able to add or remove single drives, ZFS might not be the filesystem for you. Myths and misunderstandings Information on the internet about ZFS can be outdated, conflicting or flat-out wrong. Partially this is because it has been in use for almost 15 years now and things change, partially it is the result of being used on different operating systems which have minor differences under the hood. Also, Google searches tend to first return the Oracle documentation for their closed source ZFS variant, which is increasingly diverging from the open source OpenZFS standard. To clear up some of the most common misunderstandings: No, ZFS does not need at least 8 GB of RAM This myth is especially common in FreeNAS circles . Curiously, FreeBSD, the basis of FreeNAS, will run with 1 GB . The ZFS on Linux FAQ , which is more relevant for Ansible-NAS, states under \"suggested hardware\": 8GB+ of memory for the best performance. It's perfectly possible to run with 2GB or less (and people do), but you'll need more if using deduplication. (Deduplication is only useful in special cases . If you are reading this, you probably don't need it.) Experience shows that 8 GB of RAM is in fact a sensible minimal amount for continuous use. But it's not a requirement. What everybody agrees on is that ZFS loves RAM and works better the more it has, so you should have as much of it as you possibly can. When in doubt, add more RAM, and even more, and them some, until your motherboard's capacity is reached. No, ECC RAM is not required for ZFS This is another case where a recommendation has been taken as a requirement. To quote the ZFS on Linux FAQ again: Using ECC memory for OpenZFS is strongly recommended for enterprise environments where the strongest data integrity guarantees are required. Without ECC memory rare random bit flips caused by cosmic rays or by faulty memory can go undetected. If this were to occur OpenZFS (or any other filesystem) will write the damaged data to disk and be unable to automatically detect the corruption. ECC corrects single bit errors in memory. It is always better to have it on any computer if you can afford it, and ZFS is no exception. However, there is absolutely no requirement for ZFS to have ECC RAM. If you just don't care about the danger of random bit flips because, hey, you can always just download Night of the Living Dead all over again, you're prefectly free to use normal RAM. If you do use ECC RAM, make sure your processor and motherboard support it. No, the SLOG is not really a write cache You'll read the suggestion to add a fast SSD or NVMe as a \"SLOG drive\" (mistakenly also called \"ZIL\") for write caching. This isn't what happens, because ZFS already includes a write cache in RAM. Since RAM is always faster, adding a disk as a write cache doesn't even make sense. What the ZFS Intent Log (ZIL) does, with or without a dedicated drive, is handle synchronous writes. These occur when the system refuses to signal a successful write until the data is actually stored on a physical disk somewhere. This keeps the data safe, but is slower. By default, the ZIL initially shoves a copy of the data on a normal VDEV somewhere and then gives the thumbs up. The actual write to the pool is performed later from the write cache in RAM, not the temporary copy. The data there is only ever read if the power fails before the last step. The ZIL is all about protecting data, not making transfers faster. A Separate Intent Log (SLOG) is an additional fast drive for these temporary synchronous writes. It simply allows the ZIL give the thumbs up quicker. This means that a SLOG is never read unless the power has failed before the final write to the pool. Asynchronous writes just go through the normal write cache, by the way. If the power fails, the data is gone. In summary, the ZIL prevents data loss during synchronous writes, or at least ensures that the data in storage is consistent. You always have a ZIL. A SLOG will make the ZIL faster. You'll probably need to do some research and some testing to figure out if your system would benefit from a SLOG. NFS for instance uses synchronous writes, SMB usually doesn't. When in doubt, add more RAM instead. Further reading and viewing In 2012, Aaron Toponce wrote a now slightly dated, but still very good introduction to ZFS on Linux. If you only read one part, make it the explanation of the ARC , ZFS' read cache. One of the best books on ZFS around is FreeBSD Mastery: ZFS by Michael W. Lucas and Allan Jude. Though it is written for FreeBSD, the general guidelines apply for all variants. There is a second volume for advanced use. Jeff Bonwick, one of the original creators of ZFS, tells the story of how ZFS came to be on YouTube .","title":"ZFS Overview"},{"location":"zfs/zfs_overview/#zfs-overview","text":"This is a general overview of the ZFS file system for people who are new to it. If you have some experience and are actually looking for specific information about how to configure ZFS for Ansible-NAS, check out the ZFS example configuration .","title":"ZFS Overview"},{"location":"zfs/zfs_overview/#what-is-zfs-and-why-would-i-want-it","text":"ZFS is an advanced filesystem and volume manager originally created by Sun Microsystems starting in 2001. First released in 2005 for OpenSolaris, Oracle later bought Sun and switched to developing ZFS as closed source software. An open source fork took the name OpenZFS , but is still called \"ZFS\" for short. It runs on Linux, FreeBSD, illumos and other platforms. ZFS aims to be the \"last word in filesystems\" , a technology so future-proof that Michael W. Lucas and Allan Jude famously stated that the Enterprise's computer on Star Trek probably runs it. The design was based on four principles : \"Pooled\" storage to eliminate the notion of volumes. You can add more storage the same way you just add a RAM stick to memory. Make sure data is always consistent on the disks. There is no fsck command for ZFS and none is needed. Detect and correct data corruption (\"bitrot\"). ZFS is one of the few storage systems that checksums everything, including the data itself, and is \"self-healing\". Make it easy to use. Try to \"end the suffering\" for the admins involved in managing storage. ZFS includes a host of other features such as snapshots, transparent compression and encryption. During the early years of ZFS, this all came with hardware requirements only enterprise users could afford. By now, however, computers have become so powerful that ZFS can run (with some effort) on a Raspberry Pi . FreeBSD and FreeNAS make extensive use of ZFS. What is holding ZFS back on Linux are licensing issues beyond the scope of this document. Ansible-NAS doesn't actually specify a filesystem - you can use EXT4, XFS or Btrfs as well. However, ZFS not only provides the benefits listed above, but also lets you use your hard drives with different operating systems. Some people now using Ansible-NAS came from FreeNAS, and were able to export their ZFS storage drives there and import them to Ubuntu. On the other hand, if you ever decide to switch back to FreeNAS or maybe want to use FreeBSD instead of Linux, you should be able to use the same ZFS pools.","title":"What is ZFS and why would I want it?"},{"location":"zfs/zfs_overview/#an-overview-and-some-actual-commands","text":"Storage in ZFS is organized in pools . Inside these pools, you create filesystems (also known as \"datasets\") which are like partitions on steroids. For instance, you can keep each user's /home directory in a separate filesystem. ZFS systems tend to use lots and lots of specialized filesystems with tailored parameters such as record size and compression. All filesystems share the available storage in their pool. Pools do not directly consist of hard disks or SSDs. Instead, drives are organized as virtual devices (VDEVs). This is where the physical redundancy in ZFS is located. Drives in a VDEV can be \"mirrored\" or combined as \"RaidZ\", roughly the equivalent of RAID5. These VDEVs are then combined into a pool by the administrator. The command might look something like this: sudo zpool create tank mirror /dev/sda /dev/sdb This combines /dev/sba and /dev/sdb to a mirrored VDEV, and then defines a new pool named tank consisting of this single VDEV. (Actually, you'd want to use a different ID for the drives, but you get the idea.) You can now create a filesystem in this pool for, say, all of your Mass Effect fan fiction: sudo zfs create tank/mefanfic You can then enable automatic compression on this filesystem with sudo zfs set compression=lz4 tank/mefanfic . To take a snapshot , use sudo zfs snapshot tank/mefanfic@21540411 Now, if evil people were somehow able to encrypt your precious fan fiction files with ransomware, you can simply laugh maniacally and revert to the old version: sudo zfs rollback tank/mefanfic@21540411 Of course, you would lose any texts you might have added to the filesystem between that snapshot and now. Usually, you'll have some form of automatic snapshot administration configured. To detect bitrot and other data defects, ZFS periodically runs scrubs : The system compares the available copies of each data record with their checksums. If there is a mismatch, the data is repaired.","title":"An overview and some actual commands"},{"location":"zfs/zfs_overview/#known-issues","text":"At time of writing (April 2019), ZFS on Linux does not offer native encryption, TRIM support or device removal, which are all scheduled to be included in the upcoming 0.8 release any day now. ZFS' original design for enterprise systems and redundancy requirements can make some things difficult. You can't just add individual drives to a pool and tell the system to reconfigure automatically. Instead, you have to either add a new VDEV, or replace each of the existing drives with one of higher capacity. In an enterprise environment, of course, you would just buy a bunch of new drives and move the data from the old pool to the new pool. Shrinking a pool is even harder - put simply, ZFS is not built for this, though it is being worked on . If you absolutely must be able to add or remove single drives, ZFS might not be the filesystem for you.","title":"Known issues"},{"location":"zfs/zfs_overview/#myths-and-misunderstandings","text":"Information on the internet about ZFS can be outdated, conflicting or flat-out wrong. Partially this is because it has been in use for almost 15 years now and things change, partially it is the result of being used on different operating systems which have minor differences under the hood. Also, Google searches tend to first return the Oracle documentation for their closed source ZFS variant, which is increasingly diverging from the open source OpenZFS standard. To clear up some of the most common misunderstandings:","title":"Myths and misunderstandings"},{"location":"zfs/zfs_overview/#no-zfs-does-not-need-at-least-8-gb-of-ram","text":"This myth is especially common in FreeNAS circles . Curiously, FreeBSD, the basis of FreeNAS, will run with 1 GB . The ZFS on Linux FAQ , which is more relevant for Ansible-NAS, states under \"suggested hardware\": 8GB+ of memory for the best performance. It's perfectly possible to run with 2GB or less (and people do), but you'll need more if using deduplication. (Deduplication is only useful in special cases . If you are reading this, you probably don't need it.) Experience shows that 8 GB of RAM is in fact a sensible minimal amount for continuous use. But it's not a requirement. What everybody agrees on is that ZFS loves RAM and works better the more it has, so you should have as much of it as you possibly can. When in doubt, add more RAM, and even more, and them some, until your motherboard's capacity is reached.","title":"No, ZFS does not need at least 8 GB of RAM"},{"location":"zfs/zfs_overview/#no-ecc-ram-is-not-required-for-zfs","text":"This is another case where a recommendation has been taken as a requirement. To quote the ZFS on Linux FAQ again: Using ECC memory for OpenZFS is strongly recommended for enterprise environments where the strongest data integrity guarantees are required. Without ECC memory rare random bit flips caused by cosmic rays or by faulty memory can go undetected. If this were to occur OpenZFS (or any other filesystem) will write the damaged data to disk and be unable to automatically detect the corruption. ECC corrects single bit errors in memory. It is always better to have it on any computer if you can afford it, and ZFS is no exception. However, there is absolutely no requirement for ZFS to have ECC RAM. If you just don't care about the danger of random bit flips because, hey, you can always just download Night of the Living Dead all over again, you're prefectly free to use normal RAM. If you do use ECC RAM, make sure your processor and motherboard support it.","title":"No, ECC RAM is not required for ZFS"},{"location":"zfs/zfs_overview/#no-the-slog-is-not-really-a-write-cache","text":"You'll read the suggestion to add a fast SSD or NVMe as a \"SLOG drive\" (mistakenly also called \"ZIL\") for write caching. This isn't what happens, because ZFS already includes a write cache in RAM. Since RAM is always faster, adding a disk as a write cache doesn't even make sense. What the ZFS Intent Log (ZIL) does, with or without a dedicated drive, is handle synchronous writes. These occur when the system refuses to signal a successful write until the data is actually stored on a physical disk somewhere. This keeps the data safe, but is slower. By default, the ZIL initially shoves a copy of the data on a normal VDEV somewhere and then gives the thumbs up. The actual write to the pool is performed later from the write cache in RAM, not the temporary copy. The data there is only ever read if the power fails before the last step. The ZIL is all about protecting data, not making transfers faster. A Separate Intent Log (SLOG) is an additional fast drive for these temporary synchronous writes. It simply allows the ZIL give the thumbs up quicker. This means that a SLOG is never read unless the power has failed before the final write to the pool. Asynchronous writes just go through the normal write cache, by the way. If the power fails, the data is gone. In summary, the ZIL prevents data loss during synchronous writes, or at least ensures that the data in storage is consistent. You always have a ZIL. A SLOG will make the ZIL faster. You'll probably need to do some research and some testing to figure out if your system would benefit from a SLOG. NFS for instance uses synchronous writes, SMB usually doesn't. When in doubt, add more RAM instead.","title":"No, the SLOG is not really a write cache"},{"location":"zfs/zfs_overview/#further-reading-and-viewing","text":"In 2012, Aaron Toponce wrote a now slightly dated, but still very good introduction to ZFS on Linux. If you only read one part, make it the explanation of the ARC , ZFS' read cache. One of the best books on ZFS around is FreeBSD Mastery: ZFS by Michael W. Lucas and Allan Jude. Though it is written for FreeBSD, the general guidelines apply for all variants. There is a second volume for advanced use. Jeff Bonwick, one of the original creators of ZFS, tells the story of how ZFS came to be on YouTube .","title":"Further reading and viewing"}]}